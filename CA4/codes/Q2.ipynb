{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Section 1: prepare packages and dataset","metadata":{"id":"lmcGUiPT5k_J"}},{"cell_type":"markdown","source":"## install and import libraries","metadata":{"id":"sQ9kejmc5k_O"}},{"cell_type":"code","source":"!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q trl xformers wandb datasets sentencepiece bitsandbytes evaluate","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":178515,"status":"ok","timestamp":1716532350376,"user":{"displayName":"Zahra Reyhanian","userId":"14846256199972630626"},"user_tz":-210},"id":"lyZj-pcn5k_P","outputId":"9cf8506f-0319-49c5-cf9d-bc2fd3881d62","execution":{"iopub.status.busy":"2024-05-31T08:48:28.217548Z","iopub.execute_input":"2024-05-31T08:48:28.217944Z","iopub.status.idle":"2024-05-31T08:52:53.701382Z","shell.execute_reply.started":"2024-05-31T08:48:28.217904Z","shell.execute_reply":"2024-05-31T08:52:53.700145Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall torch -y\n!pip install -q torch==2.1","metadata":{"id":"S2b1177O5k_R","outputId":"15f3966c-2795-4480-a48a-a803aac75ad8","execution":{"iopub.status.busy":"2024-05-31T08:52:53.704009Z","iopub.execute_input":"2024-05-31T08:52:53.704382Z","iopub.status.idle":"2024-05-31T08:53:49.548190Z","shell.execute_reply.started":"2024-05-31T08:52:53.704344Z","shell.execute_reply":"2024-05-31T08:53:49.547215Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.3.0\nUninstalling torch-2.3.0:\n  Successfully uninstalled torch-2.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nxformers 0.0.26.post1 requires torch==2.3.0, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q --upgrade torch torchvision","metadata":{"id":"XYOu9wuh5k_S","outputId":"e754f963-88e3-4c68-9a75-4b539718a352","execution":{"iopub.status.busy":"2024-05-31T08:53:49.549496Z","iopub.execute_input":"2024-05-31T08:53:49.549798Z","iopub.status.idle":"2024-05-31T08:54:43.786944Z","shell.execute_reply.started":"2024-05-31T08:53:49.549770Z","shell.execute_reply":"2024-05-31T08:54:43.785839Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.14 requires torch<2.3,>=1.10, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom transformers import PreTrainedModel, PretrainedConfig\nfrom transformers import AutoModel, AutoConfig\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, PromptEncoderConfig\nimport os,torch, wandb, platform, warnings\nfrom datasets import load_dataset, load_metric, Dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import AdamW,get_scheduler\nfrom tqdm.notebook import tqdm_notebook\nfrom trl import SFTTrainer\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom torch import nn\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport evaluate\nimport time\n\nfrom huggingface_hub import notebook_login\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ncompute_dtype = getattr(torch, \"float16\")","metadata":{"id":"N6iEMCEU5k_S","outputId":"9a9956ec-f65d-4919-8667-b74bea77abb2","execution":{"iopub.status.busy":"2024-05-31T08:54:43.789508Z","iopub.execute_input":"2024-05-31T08:54:43.789820Z","iopub.status.idle":"2024-05-31T08:55:02.884119Z","shell.execute_reply.started":"2024-05-31T08:54:43.789791Z","shell.execute_reply":"2024-05-31T08:55:02.883331Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-31 08:54:53.003286: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 08:54:53.003424: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 08:54:53.130990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## define the model and the dataset path","metadata":{"id":"kFjvAwPM5k_T"}},{"cell_type":"code","source":"model_name = \"meta-llama/Meta-Llama-3-8B\"\n\ndataset_name = \"nyu-mll/multi_nli\"","metadata":{"id":"RhvCivoQ5k_T","execution":{"iopub.status.busy":"2024-05-31T08:55:02.885088Z","iopub.execute_input":"2024-05-31T08:55:02.885637Z","iopub.status.idle":"2024-05-31T08:55:02.891752Z","shell.execute_reply.started":"2024-05-31T08:55:02.885612Z","shell.execute_reply":"2024-05-31T08:55:02.890875Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## login to huggingface","metadata":{"id":"Ul0c2Dp65k_T"}},{"cell_type":"code","source":"notebook_login()\ntoken='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'","metadata":{"colab":{"referenced_widgets":["d26a0c5697ca47cd884c04b87413991f"]},"id":"Mx4ExXSv5k_U","outputId":"8f8c2d4b-7e3c-47bb-91df-fd1555c2bd16","execution":{"iopub.status.busy":"2024-05-31T08:55:02.892818Z","iopub.execute_input":"2024-05-31T08:55:02.893128Z","iopub.status.idle":"2024-05-31T08:55:04.956999Z","shell.execute_reply.started":"2024-05-31T08:55:02.893104Z","shell.execute_reply":"2024-05-31T08:55:04.956074Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9c802c5c4c74da3978e5f66312d25aa"}},"metadata":{}}]},{"cell_type":"markdown","source":"## download and prepare model's tokenizer","metadata":{"id":"jMRfmLTF5k_U"}},{"cell_type":"code","source":"if any(k in model_name for k in (\"gpt\", \"opt\", \"bloom\")):\n    padding_side = \"left\"\nelse:\n    padding_side = \"right\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=padding_side, trust_remote_code=True, token=token)\nif getattr(tokenizer, \"pad_token_id\") is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"colab":{"referenced_widgets":["8bb6761eb7a64b988cfd2a46fb854098","5e18e21c0e2047209049ecf7a02189e0","a53ddefc72c54c9e8cbf33f0e00c0d63","c452ac86126f44c7a5e39eef1dbeca8b","9fcbb88f00554c83b05ab6ff9f0dea9e"]},"id":"Z3bbnoTi5k_U","outputId":"161ae3d2-6605-4ac6-8a09-c0f1fa123e99","execution":{"iopub.status.busy":"2024-05-31T08:55:04.958149Z","iopub.execute_input":"2024-05-31T08:55:04.958433Z","iopub.status.idle":"2024-05-31T08:55:08.257984Z","shell.execute_reply.started":"2024-05-31T08:55:04.958410Z","shell.execute_reply":"2024-05-31T08:55:08.257124Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc3e321e3686478baa9d8d65bdb0aa72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f28917134967434e9aa0be053ecece10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78450c1a096d420c9a1b29164c117366"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## download dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = load_dataset(dataset_name, split=\"train[:10%]\")\nval_mismatched_dataset = load_dataset(dataset_name, split=\"validation_mismatched[:40%]\")","metadata":{"colab":{"referenced_widgets":["93d98835909948b5b222f09bab2aa0f5","109f3a66536146b4baf4195ed0660ad3","0bf107990bf6465e983c1abf676c5f29","fb1bc694241f49b59f107e04148c42af"]},"id":"V2VVdhbx5k_V","outputId":"5f45f861-454a-4e33-f8d3-374a0ddc3e60","execution":{"iopub.status.busy":"2024-05-31T08:55:08.259041Z","iopub.execute_input":"2024-05-31T08:55:08.259292Z","iopub.status.idle":"2024-05-31T08:55:29.124209Z","shell.execute_reply.started":"2024-05-31T08:55:08.259269Z","shell.execute_reply":"2024-05-31T08:55:29.123290Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4f62218d6ba4756a3525bda5ff8fbac"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214M/214M [00:04<00:00, 52.5MB/s] \nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.94M/4.94M [00:00<00:00, 17.3MB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.10M/5.10M [00:00<00:00, 14.4MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0116cef2fa9427da63534840a50b36c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5e40d1ed8a447639d197c2efcda3995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618259a388544988ab517fafca288900"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Section 1: prompting","metadata":{}},{"cell_type":"markdown","source":"## evaluation function","metadata":{}},{"cell_type":"code","source":"def evaluate(y_true, y_pred):    \n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    report = classification_report(y_true, y_pred)\n    print('Classification Report:')\n    print(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## generate_prompt","metadata":{}},{"cell_type":"code","source":"def generate_prompt_for_training(example):\n    return f'''\nClassify the following sentence pairs into entailment (0), neutral (1), contradiction (2). \n{example['premise']} SEP {example['hypothesis']}\nLabel = {example['label']}'''.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_prompt(premise, hypothesis):\n    return f'''\nClassify the following sentence pairs into entailment (0), neutral (1), contradiction (2). \n{premise} SEP {hypothesis}\nLabel ='''.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[44]\n#3 10 11 15 16 21 23 44","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_prompt_one_shot(premise, hypothesis):\n    example = train_dataset[44]\n    premise0 = example['premise']\n    hypothesis0 = example['hypothesis']\n    label0 = example['label']\n    return f'''\nClassify the following sentence pairs into entailment (0), neutral (1), contradiction (2):\n{premise0} SEP {hypothesis0}\nLabel = {label0}\nNow classify the following pairs:\n{premise} SEP {hypothesis}\nLabel ='''.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, val_dataset, prompt_generator, tokenizer, i=-1, max_new_tokens=16, temprature=0.1):\n    model.eval()\n    y_true = []\n    y_pred = []\n    for sample in tqdm_notebook(val_dataset):\n        with torch.no_grad():\n            premise = sample['premise']\n            hypothesis = sample['hypothesis']\n            real_label = sample['label']\n            y_true.append(real_label)\n            prompt = prompt_generator(premise, hypothesis)\n\n            inputs = tokenizer(prompt, return_tensors=\"pt\")\n            tokens = model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=temprature,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n            answer = tokenizer.decode(tokens[0], skip_special_tokens=True).split('=')[i]\n            num = re.search(r'\\d', answer)\n            if num!=None:\n                num=num.group()\n                y_pred.append(int(num))\n            else:\n                y_pred.append(-1)\n\n    return y_true, y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## load model","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.execute_input":"2024-05-29T15:01:19.158932Z","iopub.status.busy":"2024-05-29T15:01:19.158484Z","iopub.status.idle":"2024-05-29T15:03:04.511975Z","shell.execute_reply":"2024-05-29T15:03:04.510968Z","shell.execute_reply.started":"2024-05-29T15:01:19.158903Z"},"id":"D0sc-UB1g1SC","trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a10a78c0e752472eb67051ad19a28f36","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2cea91dad92e410eaffad9af2c8fa12c","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0db8788524c41518ae43eb0448b48bc","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b0455eaf1d8413a878cc7a8cfc0a140","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19960189d56541ac8f730da0fd91e930","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38cc449e99bb4ef2a0874c5ae3c7dd8c","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4aa6110bcf10444f9e1fa70f59e43fcf","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85e65599fdca4849b633d4eda9196ed0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2561c88d7e934db9b5903e29457dfada","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":"## zero-shot","metadata":{"id":"0795FhtVg4XN"}},{"cell_type":"code","source":"y_true, y_pred = predict(model, val_mismatched_dataset, generate_prompt, tokenizer)","metadata":{"execution":{"iopub.execute_input":"2024-05-29T15:03:04.513644Z","iopub.status.busy":"2024-05-29T15:03:04.513381Z","iopub.status.idle":"2024-05-29T16:10:19.106630Z","shell.execute_reply":"2024-05-29T16:10:19.105666Z","shell.execute_reply.started":"2024-05-29T15:03:04.513622Z"},"id":"7tpH3WeFg9Hs","trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a9e662bda8343bd99505f1d48b28a63","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3933 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1725: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n\n  warnings.warn(\n"}]},{"cell_type":"code","source":"evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:10:19.109547Z","iopub.status.busy":"2024-05-29T16:10:19.109245Z","iopub.status.idle":"2024-05-29T16:10:19.140250Z","shell.execute_reply":"2024-05-29T16:10:19.139258Z","shell.execute_reply.started":"2024-05-29T16:10:19.109522Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy: 0.427\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n\n\n          -1       0.00      0.00      0.00         0\n\n           0       0.41      0.72      0.52      1435\n\n           1       0.31      0.09      0.13      1191\n\n           2       0.60      0.42      0.49      1307\n\n           8       0.00      0.00      0.00         0\n\n\n\n    accuracy                           0.43      3933\n\n   macro avg       0.26      0.24      0.23      3933\n\nweighted avg       0.44      0.43      0.39      3933\n\n\n"},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n"}]},{"cell_type":"markdown","source":"## one-shot","metadata":{"id":"3s0czW__g9Tl"}},{"cell_type":"code","source":"y_true, y_pred=predict(model, val_mismatched_dataset, generate_prompt_one_shot, tokenizer, i=2)","metadata":{"execution":{"iopub.execute_input":"2024-05-29T16:16:54.806613Z","iopub.status.busy":"2024-05-29T16:16:54.805968Z","iopub.status.idle":"2024-05-29T17:59:49.399348Z","shell.execute_reply":"2024-05-29T17:59:49.398346Z","shell.execute_reply.started":"2024-05-29T16:16:54.806583Z"},"id":"QpjfMaIehAPy","trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd12860f3fd74e61a258ddde463e8ca8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3933 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1725: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n\n  warnings.warn(\n"}]},{"cell_type":"code","source":"evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.execute_input":"2024-05-29T17:59:49.401400Z","iopub.status.busy":"2024-05-29T17:59:49.401053Z","iopub.status.idle":"2024-05-29T17:59:49.423663Z","shell.execute_reply":"2024-05-29T17:59:49.422715Z","shell.execute_reply.started":"2024-05-29T17:59:49.401357Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy: 0.457\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n\n\n          -1       0.00      0.00      0.00         0\n\n           0       0.68      0.03      0.05      1435\n\n           1       0.35      0.88      0.50      1191\n\n           2       0.78      0.55      0.64      1307\n\n\n\n    accuracy                           0.46      3933\n\n   macro avg       0.45      0.36      0.30      3933\n\nweighted avg       0.62      0.46      0.39      3933\n\n\n"},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n"}]},{"cell_type":"markdown","source":"# Section2: Fine tune with QLORA","metadata":{"id":"EZ372X2y5k_Z"}},{"cell_type":"markdown","source":"## load model","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    token=token,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.execute_input":"2024-05-30T14:27:41.666411Z","iopub.status.busy":"2024-05-30T14:27:41.665766Z","iopub.status.idle":"2024-05-30T14:29:28.870008Z","shell.execute_reply":"2024-05-30T14:29:28.869231Z","shell.execute_reply.started":"2024-05-30T14:27:41.666378Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f4e57f8a3074700bd2c8316b600a74d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"054e80f879f647f2917e5b81562fb29e","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56682076430a4c5ea660aef8896304f9","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b3253bd9bb2442bb101a0fcef1da82c","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a5624381daf417583ae905fcb005ddd","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41456ab6ebb141a69af75d8a334bf515","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33432162dd074232985be846e319bdcd","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"940a38e9d02c4de5bf05f12a13b35908","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcb1d73c79a5492486d968c3ea6226f1","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\nmodel","metadata":{"execution":{"iopub.execute_input":"2024-05-29T18:12:44.334100Z","iopub.status.busy":"2024-05-29T18:12:44.333806Z","iopub.status.idle":"2024-05-29T18:12:44.385396Z","shell.execute_reply":"2024-05-29T18:12:44.384523Z","shell.execute_reply.started":"2024-05-29T18:12:44.334075Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(128256, 4096)\n","    (layers): ModuleList(\n","      (0-31): 32 x LlamaDecoderLayer(\n","        (self_attn): LlamaSdpaAttention(\n","          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",")"]},"metadata":{}}]},{"cell_type":"markdown","source":"## LoRA config","metadata":{"id":"_Xe-WawA5k_e"}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\npeft_config = LoraConfig(\n    lora_alpha= 16,\n    lora_dropout= 0.1,\n    r= 64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n)\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.execute_input":"2024-05-29T18:12:44.386928Z","iopub.status.busy":"2024-05-29T18:12:44.386578Z","iopub.status.idle":"2024-05-29T18:12:46.159990Z","shell.execute_reply":"2024-05-29T18:12:46.159062Z","shell.execute_reply.started":"2024-05-29T18:12:44.386898Z"},"id":"J1M1WQQ45k_e","trusted":true},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"trainable params: 130,023,424 || all params: 8,160,284,672 || trainable%: 1.5934\n"}]},{"cell_type":"markdown","source":"## preprocess data","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame(train_dataset).apply(generate_prompt_for_training, axis=1)\ntrain_data = pd.DataFrame(temp, columns=['text'])\n\ntrain_data = Dataset.from_pandas(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## training arguments and train","metadata":{"id":"oyPvmOyl5k_f"}},{"cell_type":"code","source":"output_dir = \"results\"\nos.mkdir(output_dir)","metadata":{"execution":{"iopub.execute_input":"2024-05-29T18:12:52.320146Z","iopub.status.busy":"2024-05-29T18:12:52.319790Z","iopub.status.idle":"2024-05-29T18:12:52.324872Z","shell.execute_reply":"2024-05-29T18:12:52.323896Z","shell.execute_reply.started":"2024-05-29T18:12:52.320115Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"EPOCHS=1\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    save_strategy=\"epoch\",\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    logging_steps= 30,\n    max_steps=1000,\n    warmup_steps=4,\n    warmup_ratio=0.03,\n    learning_rate=2e-4,\n    weight_decay= 0.001,\n    fp16= False,\n    bf16= False,\n    max_grad_norm= 0.3,\n    group_by_length= False,\n    optim=\"paged_adamw_8bit\",\n    lr_scheduler_type= \"constant\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    dataset_text_field=\"text\",\n    packing=False,\n    dataset_kwargs={\n        \"add_special_tokens\": False,\n        \"append_concat_token\": False,\n    },\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"execution":{"iopub.execute_input":"2024-05-29T18:12:52.326301Z","iopub.status.busy":"2024-05-29T18:12:52.325952Z","iopub.status.idle":"2024-05-29T18:12:57.819974Z","shell.execute_reply":"2024-05-29T18:12:57.819039Z","shell.execute_reply.started":"2024-05-29T18:12:52.326276Z"},"id":"F0pbUF415k_f","trusted":true},"execution_count":18,"outputs":[{"name":"stderr","output_type":"stream","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n\n  warnings.warn(\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1740949ce2c41f9996c1286e919d945","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/39270 [00:00<?, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"max_steps is given, it will override any value given in num_train_epochs\n"}]},{"cell_type":"code","source":"model.train()\ntrainer.train()\n\n# 4440f4c955c9751dc1828aaf1da0eaf84473d21e","metadata":{"execution":{"iopub.execute_input":"2024-05-29T18:12:57.821902Z","iopub.status.busy":"2024-05-29T18:12:57.821244Z","iopub.status.idle":"2024-05-29T19:52:30.359243Z","shell.execute_reply":"2024-05-29T19:52:30.358302Z","shell.execute_reply.started":"2024-05-29T18:12:57.821865Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"},{"name":"stdout","output_type":"stream","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"},{"output_type":"display_data","data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240529_181619-h5rh1384</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/univerityoftehran/huggingface/runs/h5rh1384' target=\"_blank\">results</a></strong> to <a href='https://wandb.ai/univerityoftehran/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View project at <a href='https://wandb.ai/univerityoftehran/huggingface' target=\"_blank\">https://wandb.ai/univerityoftehran/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View run at <a href='https://wandb.ai/univerityoftehran/huggingface/runs/h5rh1384' target=\"_blank\">https://wandb.ai/univerityoftehran/huggingface/runs/h5rh1384</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n\n  warnings.warn(\n"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1000/1000 1:35:46, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>30</td>\n","      <td>2.142200</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.678400</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.665900</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.678800</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.649700</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>1.653400</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>1.563000</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>1.655700</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>1.575700</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.592400</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>1.634300</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>1.565200</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>1.602300</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>1.614600</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.650100</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>1.564000</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>1.608000</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>1.605200</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>1.571800</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.608100</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>1.610700</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>1.639700</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>1.624400</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>1.588500</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.601700</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>1.575100</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>1.540300</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>1.628800</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>1.638200</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.600800</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>1.602300</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>1.601400</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>1.567000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1000, training_loss=1.6257954626083373, metrics={'train_runtime': 5972.0023, 'train_samples_per_second': 0.67, 'train_steps_per_second': 0.167, 'total_flos': 1.1285629067894784e+16, 'train_loss': 1.6257954626083373, 'epoch': 0.10185892538833716})"]},"metadata":{}}]},{"cell_type":"code","source":"model = model.merge_and_unload()","metadata":{"execution":{"iopub.execute_input":"2024-05-29T19:52:34.047820Z","iopub.status.busy":"2024-05-29T19:52:34.047440Z","iopub.status.idle":"2024-05-29T19:52:56.628610Z","shell.execute_reply":"2024-05-29T19:52:56.627661Z","shell.execute_reply.started":"2024-05-29T19:52:34.047790Z"},"id":"kaVwQVB_5k_f","trusted":true},"execution_count":20,"outputs":[{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n\n  warnings.warn(\n"}]},{"cell_type":"code","source":"y_true, y_pred = predict(model, val_mismatched_dataset, generate_prompt, tokenizer)","metadata":{"execution":{"iopub.execute_input":"2024-05-29T19:53:01.371772Z","iopub.status.busy":"2024-05-29T19:53:01.371409Z","iopub.status.idle":"2024-05-29T20:41:30.978496Z","shell.execute_reply":"2024-05-29T20:41:30.977453Z","shell.execute_reply.started":"2024-05-29T19:53:01.371744Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c70994bfc8914d48a8a7c647ff115bee","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3933 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1725: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n\n  warnings.warn(\n"}]},{"cell_type":"code","source":"evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.execute_input":"2024-05-29T20:41:30.980835Z","iopub.status.busy":"2024-05-29T20:41:30.980449Z","iopub.status.idle":"2024-05-29T20:41:31.013142Z","shell.execute_reply":"2024-05-29T20:41:31.012085Z","shell.execute_reply.started":"2024-05-29T20:41:30.980798Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy: 0.479\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n\n\n          -1       0.00      0.00      0.00         0\n\n           0       0.73      0.31      0.43      1435\n\n           1       0.39      0.17      0.24      1191\n\n           2       0.46      0.95      0.62      1307\n\n\n\n    accuracy                           0.48      3933\n\n   macro avg       0.39      0.36      0.32      3933\n\nweighted avg       0.54      0.48      0.43      3933\n\n\n"},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n"}]},{"cell_type":"markdown","source":"# Section 3: fine tune with QLORA (part II)","metadata":{}},{"cell_type":"markdown","source":"## create custom model","metadata":{}},{"cell_type":"code","source":"class MyConfig(PretrainedConfig):\n    model_type = 'mymodel'\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:55:29.125626Z","iopub.execute_input":"2024-05-31T08:55:29.126075Z","iopub.status.idle":"2024-05-31T08:55:29.131647Z","shell.execute_reply.started":"2024-05-31T08:55:29.126036Z","shell.execute_reply":"2024-05-31T08:55:29.130582Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class CustomLLamaModel(PreTrainedModel):\n    config_class = MyConfig\n    def __init__(self, config, model, num_labels): \n        super().__init__(config) \n        self.config = config\n        self.num_labels = num_labels \n\n        #Load Model \n        self.model = model = model\n        self.input_dim = model.config.vocab_size\n        self.dropout = nn.Dropout(0.1) \n        self.classifier = nn.Linear(self.input_dim, self.num_labels) # load and initialize weights\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None, return_dict=None, **kwargs):\n        #Extract outputs from the body\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n\n        #Add custom layers\n        sequence_output = self.dropout(outputs[0]) \n        \n        logits = self.classifier(sequence_output[:,0,:]) \n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(labels.shape[0], self.num_labels), labels)\n\n        return {\"loss\":loss, \"logits\":logits}","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:55:29.133837Z","iopub.execute_input":"2024-05-31T08:55:29.134110Z","iopub.status.idle":"2024-05-31T08:55:29.167254Z","shell.execute_reply.started":"2024-05-31T08:55:29.134086Z","shell.execute_reply":"2024-05-31T08:55:29.166368Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## load model","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    token=token,\n    device_map='auto',\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.status.busy":"2024-05-31T08:55:29.168288Z","iopub.execute_input":"2024-05-31T08:55:29.168613Z","iopub.status.idle":"2024-05-31T09:02:25.558053Z","shell.execute_reply.started":"2024-05-31T08:55:29.168581Z","shell.execute_reply":"2024-05-31T09:02:25.557288Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d89ada09624c7a8137ae9497dc7b80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c5bbd27d484a738f8731450323e94f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"599ed41a203e4d0e89dabac65cadeecf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"775bc96f867344bc99ecb74c94f9f8ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcdcdfeaee3a4e31a52768eabefea063"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbd2d7a373e149ebbeebe18c47f77e20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb009a6f3f3e472c804d96ae6b47580e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1f9fc4f74b04df19d4b6ced15ee2b0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b56018934d7243908fbc868ff56881da"}},"metadata":{}}]},{"cell_type":"code","source":"config = MyConfig()\ncustom_model = CustomLLamaModel(config,model, num_labels=3)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:02:25.559897Z","iopub.execute_input":"2024-05-31T09:02:25.560186Z","iopub.status.idle":"2024-05-31T09:02:25.583996Z","shell.execute_reply.started":"2024-05-31T09:02:25.560161Z","shell.execute_reply":"2024-05-31T09:02:25.583237Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"custom_model = prepare_model_for_kbit_training(custom_model)\ncustom_model","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:02:25.585001Z","iopub.execute_input":"2024-05-31T09:02:25.585271Z","iopub.status.idle":"2024-05-31T09:02:25.665642Z","shell.execute_reply.started":"2024-05-31T09:02:25.585247Z","shell.execute_reply":"2024-05-31T09:02:25.664839Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"CustomLLamaModel(\n  (model): LlamaForCausalLM(\n    (model): LlamaModel(\n      (embed_tokens): Embedding(128256, 4096)\n      (layers): ModuleList(\n        (0-31): 32 x LlamaDecoderLayer(\n          (self_attn): LlamaSdpaAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (rotary_emb): LlamaRotaryEmbedding()\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm()\n          (post_attention_layernorm): LlamaRMSNorm()\n        )\n      )\n      (norm): LlamaRMSNorm()\n    )\n    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=128256, out_features=3, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha= 16,\n    lora_dropout= 0.1,\n    r= 64,\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n)\n\ncustom_model = get_peft_model(custom_model, peft_config)\ncustom_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:02:25.666660Z","iopub.execute_input":"2024-05-31T09:02:25.666923Z","iopub.status.idle":"2024-05-31T09:02:27.476557Z","shell.execute_reply.started":"2024-05-31T09:02:25.666896Z","shell.execute_reply":"2024-05-31T09:02:27.475560Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"trainable params: 130,408,195 || all params: 8,161,054,214 || trainable%: 1.5979\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## preprocess dataset","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    # max_length=None => use the model max length (it's actually the default)\n    outputs = tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, max_length=None)\n    return outputs\n\nremove_columns = ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre']","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:02:27.478008Z","iopub.execute_input":"2024-05-31T09:02:27.478564Z","iopub.status.idle":"2024-05-31T09:02:27.484336Z","shell.execute_reply.started":"2024-05-31T09:02:27.478531Z","shell.execute_reply":"2024-05-31T09:02:27.483624Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = train_dataset.map(tokenize_function, batched=True, remove_columns = remove_columns, batch_size=1)\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:02:27.486799Z","iopub.execute_input":"2024-05-31T09:02:27.487116Z","iopub.status.idle":"2024-05-31T09:03:14.166258Z","shell.execute_reply.started":"2024-05-31T09:02:27.487092Z","shell.execute_reply":"2024-05-31T09:03:14.165258Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39270 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fef9d5da78d44e819c10b16b422ffd93"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 39270\n})"},"metadata":{}}]},{"cell_type":"code","source":"val_tokenized_datasets = val_mismatched_dataset.map(tokenize_function, batched=True, remove_columns = remove_columns, batch_size=1)\nval_tokenized_datasets = val_tokenized_datasets.rename_column(\"label\", \"labels\")","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:14.167422Z","iopub.execute_input":"2024-05-31T09:03:14.167720Z","iopub.status.idle":"2024-05-31T09:03:19.217290Z","shell.execute_reply.started":"2024-05-31T09:03:14.167694Z","shell.execute_reply":"2024-05-31T09:03:19.216380Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3933 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db98bb036907406aa00d36f70d6d967c"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator=transformers.DataCollatorWithPadding(tokenizer, padding=True, max_length=120)\ntrain_dataloader = DataLoader(\n    tokenized_datasets, shuffle=True, batch_size=1, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    val_tokenized_datasets, batch_size=2, collate_fn=data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:19.219354Z","iopub.execute_input":"2024-05-31T09:03:19.219705Z","iopub.status.idle":"2024-05-31T09:03:19.226803Z","shell.execute_reply.started":"2024-05-31T09:03:19.219672Z","shell.execute_reply":"2024-05-31T09:03:19.226043Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## train model","metadata":{}},{"cell_type":"code","source":"metric = load_metric(\"accuracy\", trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:31:07.293752Z","iopub.execute_input":"2024-05-31T10:31:07.294162Z","iopub.status.idle":"2024-05-31T10:31:08.249480Z","shell.execute_reply.started":"2024-05-31T10:31:07.294130Z","shell.execute_reply":"2024-05-31T10:31:08.248711Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(custom_model.parameters(), lr=2e-5)\nnum_epochs=1\nnum_training_steps=5000\nlog_step=100\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=1000,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:21.593144Z","iopub.execute_input":"2024-05-31T09:03:21.593397Z","iopub.status.idle":"2024-05-31T09:03:21.611334Z","shell.execute_reply.started":"2024-05-31T09:03:21.593374Z","shell.execute_reply":"2024-05-31T09:03:21.610499Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"custom_model = custom_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:21.612623Z","iopub.execute_input":"2024-05-31T09:03:21.613313Z","iopub.status.idle":"2024-05-31T09:03:21.639404Z","shell.execute_reply.started":"2024-05-31T09:03:21.613279Z","shell.execute_reply":"2024-05-31T09:03:21.638557Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    custom_model.train()\n    i=0\n    for batch in tqdm_notebook(train_dataloader, total=num_training_steps):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        outputs = custom_model(**batch)\n        loss = outputs['loss']\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        i+=1\n        if i%log_step==0:\n            print(f'step {i} loss= {loss.item()}')\n            time.sleep(1)\n        if i>num_training_steps:\n            break","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:21.640471Z","iopub.execute_input":"2024-05-31T09:03:21.641360Z","iopub.status.idle":"2024-05-31T10:11:33.177650Z","shell.execute_reply.started":"2024-05-31T09:03:21.641327Z","shell.execute_reply":"2024-05-31T10:11:33.176718Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c862c07f2f4cfeb933626dcde6d429"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2724: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"step 100 loss= 1.1972627639770508\nstep 200 loss= 4.992378234863281\nstep 300 loss= 4.608397960662842\nstep 400 loss= 2.3926708698272705\nstep 500 loss= 0.3174169659614563\nstep 600 loss= 1.5125761032104492\nstep 700 loss= 1.8707401752471924\nstep 800 loss= 1.789634346961975\nstep 900 loss= 1.0456010103225708\nstep 1000 loss= 1.2383553981781006\nstep 1100 loss= 1.6074318885803223\nstep 1200 loss= 0.9824433922767639\nstep 1300 loss= 1.1715129613876343\nstep 1400 loss= 0.7716953754425049\nstep 1500 loss= 1.0802257061004639\nstep 1600 loss= 0.8927186131477356\nstep 1700 loss= 1.3111153841018677\nstep 1800 loss= 1.0205429792404175\nstep 1900 loss= 1.2436267137527466\nstep 2000 loss= 0.9533219337463379\nstep 2100 loss= 1.3187408447265625\nstep 2200 loss= 1.4149916172027588\nstep 2300 loss= 1.2637218236923218\nstep 2400 loss= 0.9347504377365112\nstep 2500 loss= 1.399399995803833\nstep 2600 loss= 1.217358946800232\nstep 2700 loss= 1.5362646579742432\nstep 2800 loss= 1.4145417213439941\nstep 2900 loss= 1.5519506931304932\nstep 3000 loss= 0.9945833086967468\nstep 3100 loss= 0.9655798673629761\nstep 3200 loss= 0.891904354095459\nstep 3300 loss= 1.0290911197662354\nstep 3400 loss= 1.2251578569412231\nstep 3500 loss= 1.1084139347076416\nstep 3600 loss= 0.996017575263977\nstep 3700 loss= 1.2005343437194824\nstep 3800 loss= 1.0516501665115356\nstep 3900 loss= 0.9690894484519958\nstep 4000 loss= 1.110788345336914\nstep 4100 loss= 0.8604851365089417\nstep 4200 loss= 1.1647483110427856\nstep 4300 loss= 1.0584148168563843\nstep 4400 loss= 0.9611567854881287\nstep 4500 loss= 0.9032073020935059\nstep 4600 loss= 1.3168141841888428\nstep 4700 loss= 0.8629046082496643\nstep 4800 loss= 1.2517249584197998\nstep 4900 loss= 0.9648616313934326\nstep 5000 loss= 0.7210728526115417\n","output_type":"stream"}]},{"cell_type":"code","source":"custom_model.eval()\nfor batch in tqdm_notebook(eval_dataloader):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = custom_model(**batch)\n\n    logits = outputs['logits']\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])  \n\n\nprint(metric.compute())","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:38:13.137431Z","iopub.execute_input":"2024-05-31T10:38:13.137833Z","iopub.status.idle":"2024-05-31T10:56:52.305175Z","shell.execute_reply.started":"2024-05-31T10:38:13.137802Z","shell.execute_reply":"2024-05-31T10:56:52.304180Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1967 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3615e46518f94f1ca1178cc1c6035057"}},"metadata":{}},{"name":"stdout","text":"{'accuracy': 0.33438556933483654}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}